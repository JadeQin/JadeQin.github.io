<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://JadeQin.github.io/</id>
    <title>技术随笔</title>
    <updated>2019-06-15T09:06:02.383Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://JadeQin.github.io/"/>
    <link rel="self" href="https://JadeQin.github.io//atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://JadeQin.github.io//images/avatar.png</logo>
    <icon>https://JadeQin.github.io//favicon.ico</icon>
    <rights>All rights reserved 2019, 技术随笔</rights>
    <entry>
        <title type="html"><![CDATA[Kubernetes 服务发现 — kube-dns]]></title>
        <id>https://JadeQin.github.io//post/20180521-kubedns</id>
        <link href="https://JadeQin.github.io//post/20180521-kubedns">
        </link>
        <updated>2018-05-20T23:07:46.000Z</updated>
        <content type="html"><![CDATA[<h2 id="服务发现">服务发现</h2>
<p>kubernetes 提供了 service 的概念可以通过 VIP 访问 pod 提供的服务，但是在使用的时候还有一个问题：怎么知道某个应用的 VIP？比如我们有两个应用，一个 app，一个 是 db，每个应用使用 rc 进行管理，并通过 service 暴露出端口提供服务。app 需要连接到 db 应用，我们只知道 db 应用的名称，但是并不知道它的 VIP 地址。</p>
<p>最简单的办法是从 kubernetes 提供的 API 查询。但这是一个糟糕的做法，首先每个应用都要在启动的时候编写查询依赖服务的逻辑，这本身就是重复和增加应用的复杂度；其次这也导致应用需要依赖 kubernetes，不能够单独部署和运行（当然如果通过增加配置选项也是可以做到的，但这又是增加负责度）。</p>
<p>开始的时候，kubernetes 采用了 docker 使用过的方法——环境变量。每个 pod 启动时候，会把通过环境变量设置所有服务的 IP 和 port 信息，这样 pod 中的应用可以通过读取环境变量来获取依赖服务的地址信息。这种方式服务和环境变量的匹配关系有一定的规范，使用起来也相对简单，但是有个很大的问题：依赖的服务必须在 pod 启动之前就存在，不然是不会出现在环境变量中的。</p>
<p>更理想的方案是：应用能够直接使用服务的名字，不需要关心它实际的 ip 地址，中间的转换能够自动完成。名字和 ip 之间的转换就是 DNS 系统的功能，因此 kubernetes 也提供了 DNS 方法来解决这个问题。</p>
<h2 id="部署-dns-服务">部署 DNS 服务</h2>
<p>DNS 服务不是独立的系统服务，而是一种 addon ，作为插件来安装的，不是 kubernetes 集群必须的（但是非常推荐安装）。可以把它看做运行在集群上的应用，只不过这个应用比较特殊而已。</p>
<p>DNS 有两种配置方式，在 1.3 之前使用 etcd + kube2sky + skydns 的方式，在 1.3 之后可以使用 kubedns + dnsmasq 的方式。</p>
<h4 id="修改-kubelet-启动参数">修改 kubelet 启动参数</h4>
<p>不管以什么方式启动，对外的效果是一样的。要想使用 DNS 功能，还需要修改 kubelet 的启动配置项，告诉 kubelet，给每个启动的 pod 设置对应的 DNS 信息，一共有两个参数：--cluster_dns=10.10.10.10 --cluster_domain=cluster.local，分别是 DNS 在集群中的 vip 和域名后缀，要和 DNS rc 中保持一致。</p>
<p>下面是这种方式的部署配置文件：</p>
<pre><code>apiVersion: v1
kind: ReplicationController
metadata:
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: &quot;true&quot;
  name: kube-dns
  namespace: kube-system
spec:
  replicas: 1
  selector:
    k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
        kubernetes.io/cluster-service: &quot;true&quot;
    spec:
      containers:
        - name: etcd
          command:
            - /usr/local/bin/etcd
            - &quot;-listen-client-urls&quot;
            - &quot;http://127.0.0.1:2379,http://127.0.0.1:4001&quot;
            - &quot;-advertise-client-urls&quot;
            - &quot;http://127.0.0.1:2379,http://127.0.0.1:4001&quot;
            - &quot;-initial-cluster-token&quot;
            - skydns-etcd
          image: &quot;gcr.io/google_containers/etcd:2.0.9&quot;
          resources:
            limits:
              cpu: 100m
              memory: 50Mi
        - name: kube2sky
          args:
            - &quot;-domain=cluster.local&quot;
            - &quot;-kube_master_url=http://10.7.114.81:8080&quot;
          image: &quot;gcr.io/google_containers/kube2sky:1.11&quot;
          resources:
            limits:
              cpu: 100m
              memory: 50Mi
        - name: skydns
          args:
            - &quot;-machines=http://localhost:4001&quot;
            - &quot;-addr=0.0.0.0:53&quot;
            - &quot;-domain=cluster.local&quot;
          image: &quot;gcr.io/google_containers/skydns:2015-03-11-001&quot;
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - &quot;-c&quot;
                - &quot;nslookup kubernetes.default.svc.cluster.local localhost &gt;/dev/null&quot;
            initialDelaySeconds: 30
            timeoutSeconds: 5
          ports:
            - containerPort: 53
              name: dns
              protocol: UDP
            - containerPort: 53
              name: dns-tcp
              protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 50Mi
      dnsPolicy: Default
</code></pre>
<p>这里有两个需要根据实际情况配置的地方：</p>
<p><code>kube_master_url</code>： kube2sky 会用到 kubernetes master API，它会读取里面的 service 信息</p>
<p><code>domain</code>：域名后缀，默认是 cluster.local，你可以根据实际需要修改成任何合法的值</p>
<p>然后是 Service 的配置文件：</p>
<pre><code>apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: &quot;true&quot;
    kubernetes.io/name: &quot;KubeDNS&quot;
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: 10.10.10.10
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
</code></pre>
<p>这里需要注意的是 clusterIP: 10.10.10.10 这一行手动指定了 DNS service 的 IP 地址，这个地址必须在预留的 vip 网段。手动指定的原因是为了固定这个 ip，这样启动 kubelet 的时候配置 --cluster_dns=10.10.10.10 比较方便，不需要再动态获取 DNS 的 vip 地址。</p>
<p>有了这两个文件，直接创建对象就行：</p>
<pre><code>$ kubectl create -f ./skydns-rc.yml
$ kubectl create -f ./skydns-svc.yml
[root@localhost ~]# kubectl get svc,rc,pod --namespace=kube-system
NAME                     CLUSTER-IP     EXTERNAL-IP   PORT(S)                         AGE
svc/kube-dns             10.10.10.10    &lt;none&gt;        53/UDP                          1d

NAME          DESIRED   CURRENT   READY     AGE
rc/kube-dns   1         1         1         41m

NAME                READY     STATUS    RESTARTS   AGE
po/kube-dns-twl0q   3/3       Running   0          41m
</code></pre>
<h2 id="kubedns">kubeDNS</h2>
<p>在 kubernetes 1.3 版本之后，kubernetes 改变了 DNS 的部署方式，变成了 kubeDNS + dnsmasq，没有了 etcd 。在这种模式下，kubeDNS 是原来 kube2sky + skyDNS + etcd，只不过它把数据都保存到自己的内存，而不是 kv store 中；dnsmasq 的引进是为了提高解析的速度，因为它可以配置 DNS 缓存。</p>
<p>这种部署方式的完整配置文件这里就不贴出来了，我放到了 github gist 上面，有兴趣可以查看。创建方法也是一样 kubectl create -f ./skydns-rc.yml</p>
<h2 id="测试-dns-可用性">测试 DNS 可用性</h2>
<p>不管那种部署方式，kubernetes 对外提供的 DNS 服务是一致的。每个 service 都会有对应的 DNS 记录，kubernetes 保存 DNS 记录的格式如下：</p>
<p><code>&lt;service_name&gt;.&lt;namespace&gt;.svc.&lt;domain&gt;</code></p>
<p>每个部分的字段意思：</p>
<p><code>service_name</code>: 服务名称，就是定义 service 的时候取的名字</p>
<p><code>namespace</code>：service 所在 namespace 的名字</p>
<p><code>domain</code>：提供的域名后缀，比如默认的 cluster.local</p>
<p>在 pod 中可以通过 service_name.namespace.svc.domain 来访问任何的服务，也可以使用缩写 service_name.namespace，如果 pod 和 service 在同一个 namespace，甚至可以直接使用 service_name。</p>
<blockquote>
<p>正常的 service 域名会被解析成 service vip，而 headless service 域名会被直接解析成背后的 pods ip。</p>
</blockquote>
<p>虽然不会经常用到，但是 pod 也会有对应的 DNS 记录，格式是 pod-ip-address.<namespace>.pod.<domain>，其中 pod-ip-address 为 pod ip 地址的用 - 符号隔开的格式，比如 pod ip 地址是 1.2.3.4 ，那么对应的域名就是 1-2-3-4.default.pod.cluster.local。</p>
<p>我们运行一个 busybox 来验证 DNS 服务能够正常工作：</p>
<pre><code>/ # nslookup whoami
Server:    10.10.10.10
Address 1: 10.10.10.10

Name:      whoami
Address 1: 10.10.10.175

/ # nslookup kubernetes
Server:    10.10.10.10
Address 1: 10.10.10.10

Name:      kubernetes
Address 1: 10.10.10.1

/ # nslookup whoami.default.svc
Server:    10.10.10.10
Address 1: 10.10.10.10

Name:      whoami.default.svc
Address 1: 10.10.10.175

/ # nslookup whoami.default.svc.transwarp.local
Server:    10.10.10.10
Address 1: 10.10.10.10

Name:      whoami.default.svc.transwarp.local
Address 1: 10.10.10.175
</code></pre>
<p>可以看出，如果我们在默认的 namespace default 创建了名为 whoami 的服务，以下所有域名都能被正确解析：</p>
<pre><code>whoami
whoami.default.svc
whoami.default.svc.cluster.local
</code></pre>
<p>每个 pod 的 DNS 配置文件如下，可以看到 DNS vip 地址以及搜索的 domain 列表：</p>
<pre><code>/ # cat /etc/resolv.conf
search default.pod.cluster.local default.svc.cluster.local svc.cluster.local cluster.local
nameserver 10.10.10.10
options ndots:5
options ndots:5
</code></pre>
<h2 id="kubernetes-dns-原理解析">kubernetes DNS 原理解析</h2>
<p>我们前面介绍了两种不同 DNS 部署方式，这部分讲讲它们内部的原理。</p>
<h3 id="kube2sky-模式">kube2sky 模式</h3>
<p>这种模式下主要有三个容器在运行：</p>
<pre><code>[root@localhost ~]# docker ps
CONTAINER ID        IMAGE                                              COMMAND                  CREATED             STATUS              PORTS                                          NAMES
919cbc006da2        172.16.1.41:5000/google_containers/kube2sky:1.12   &quot;/kube2sky /kube2sky &quot;   About an hour ago   Up About an hour                                                   k8s_kube2sky.80a41edc_kube-dns-twl0q_kube-system_ea1f5f4d-15cf-11e7-bece-080027c09e5b_1bd3fdb4
73dd11cac057        172.16.1.41:5000/jenkins/etcd:live                 &quot;etcd -data-dir=/var/&quot;   About an hour ago   Up About an hour                                                   k8s_etcd.4040370_kube-dns-twl0q_kube-system_ea1f5f4d-15cf-11e7-bece-080027c09e5b_b0e5a99f
0b10ae639989        172.16.1.41:5000/jenkins/skydns:20150703-113305    &quot;bootstrap.sh&quot;           About an hour ago   Up About an hour                                                   k8s_skydns.73baf3b1_kube-dns-twl0q_kube-system_ea1f5f4d-15cf-11e7-bece-080027c09e5b_2860aa6d
</code></pre>
<p>这三个容器的作用分别是：</p>
<p><code>etcd</code>：保存所有的 DNS 数据</p>
<p><code>kube2sky</code>： 通过 kubernetes API 监听 Service 的变化，然后同步到 etcd</p>
<p><code>skyDNS</code>：根据 etcd 中的数据，对外提供 DNS 查询服务</p>
<p><img src="https://ww3.sinaimg.cn/large/006tNc79gy1feisvkwauej30p00a1gm0.jpg" alt="img1"></p>
<h3 id="kubedns-模式">kubeDNS 模式</h3>
<p>这种模式下，kubeDNS 容器替代了原来的三个容器的功能，它会监听 apiserver 并把所有 service 和 endpoints 的结果在内存中用合适的数据结构保存起来，并对外提供 DNS 查询服务。</p>
<p><img src="https://ww2.sinaimg.cn/large/006tNbRwgy1feiswjz6hgj30p00a174j.jpg" alt="img2"></p>
<p><code>kubeDNS</code>：提供了原来 kube2sky + etcd + skyDNS 的功能，可以单独对外提供 DNS 查询服务</p>
<p><code>dnsmasq</code>： 一个轻量级的 DNS 服务软件，可以提供 DNS 缓存功能。kubeDNS 模式下，dnsmasq 在内存中预留一块大小（默认是 1G）的地方，保存当前最常用的 DNS 查询记录，如果缓存中没有要查找的记录，它会到 kubeDNS 中查询，并把结果缓存起来</p>
<p>每种模式都可以运行额外的 exec-healthz 容器对外提供 health check 功能，证明当前 DNS 服务是正常的。</p>
<p>exec-healthz：运行某个命令，根据结果来对外提供 /healthz 结果</p>
<h2 id="总结">总结</h2>
<p>推荐使用 kubeDNS 的模式来部署，因为它有着以下的好处：</p>
<ul>
<li>不需要额外的存储，省去了额外的维护和数据保存的工作</li>
<li>更好的性能。通过 dnsmasq 缓存和直接把 DNS 记录保存在内存中，来提高 DNS 解析的速度</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Java Module System State]]></title>
        <id>https://JadeQin.github.io//post/java-module-system-state</id>
        <link href="https://JadeQin.github.io//post/java-module-system-state">
        </link>
        <updated>2015-11-10T23:08:15.000Z</updated>
        <content type="html"><![CDATA[<h2 id="摘要">摘要：</h2>
<p>Oracle Java平台组的首席架构师Mark Reinhold，公开了一份关于模块系统的状况的报告，同时强调了其目标，并解释了如何满足当前的需要。由于和已经存在的框架存在明显的重复，特别是OSGi，报告引起发了人们的讨论。InfoQ回顾了这份经历及其当前的状况。</p>
<h2 id="正文">正文：</h2>
<p>Oracle Java平台组首席架构师Mark Reinhold发表了一份关于模块化系统的情况的报告，强调了模块化的目标是什么。由于和已经存在的框架存在明显的重复，特别是OSGi，报告引起发了人们的讨论。</p>
<p>正如报告中所解释的，以及在JSR-376和模块化系统项目主页中完整的详细说明，模块化系统是为了解决当前Java访问模型中的两个疏漏：</p>
<ul>
<li>可靠的配置：当前一个组件通过类路径访问另一个组件的类时相当容易出错，特别是尝试使用不在类路径里面或者存在多个版本的类的时候。</li>
<li>强封装：没有办法去限制特定组件暴露给其他组件的类，外部能访问所有的公共类。</li>
</ul>
<p>完整的细节能够在报告和InfoQ之前的文章中找到，总的来说，每一个组件通常（但不一定）作为一个jar文件，其中包含了如下结构的模块描述文件module-info.java：</p>
<pre><code class="language-java">module com.foo.bar {

   requires com.foo.baz;

   exports com.foo.bar.alpha;

   exports com.foo.bar.beta;

}
</code></pre>
<p>文件结构中包含一行或多行exports用于指出能够被其他组件访问的包，零行或多行requires用于指出自身需要访问的其他模块。该系统提供了方法用于在编译时评估访问类型是否具有正确的可见性（例如声明为公共类并被所需组件导出），并在运行时评估必需的模块是否可用，而不需要去检查完整的类路径。类似于OSGi中的清单文件。</p>
<h4 id="osgi的背景">OSGi的背景</h4>
<p>OSGi是一个基于Java的模块化系统和服务化平台，实现了一个完整的动态的组件模型。从1998年在JSR-8第一次提出，在随后的审核中被延迟发布（最近一次是2014年），OSGi定义了bundle（类似模块），采用包含如下的MANIFEST.MF文件的JAR文件的形式：</p>
<pre><code class="language-java">Bundle-Name: Hello World

Bundle-SymbolicName: org.wikipedia.helloworld

Bundle-Description: A Hello World bundle

Bundle-ManifestVersion: 2

Bundle-Version: 1.0.0

Bundle-Activator: org.wikipedia.Activator

Export-Package: org.wikipedia.helloworld;version=&quot;1.0.0&quot;

Import-Package: org.osgi.framework;version=&quot;1.3.0&quot;
</code></pre>
<p>（例子来源于Wikipedia）</p>
<p>显而易见的，尽管格式不一样，但是目的表现的和Java模块系统很相似。的确，Java平台模块系统和OSGi之间的相似性，在2005年提出的JSR-277，“Java模块系统”中，初次尝试模块化Java时就已经被注意到了。最初瞄准的Java 7，JSR-277专注简化分发和执行Java程序包。尽管有和JSR-376几乎一样的名字，但两者最初的目标有着细微差别；虽然它的任务是修复“可靠的配置”问题，但是它并不试图解决“强封装”的问题。相对于JSR-376，它也尝试增加一个版本模型到Java程序包中。这些目标和OSGi提供的功能之间的相似性是完全足够让作者在最初考虑把OSGi作为一个解决方案，放弃的原因考虑为OSGi的版本控制太弱了。</p>
<p>不久之后创建的JSR-294的目标是实现“改进Java程序语言的模块性支持”。同时针对Java 7，JSR-294新增了模块概念（所谓的“超级包”）来修复强封装的问题；这个概念匹配了当前的Java平台模块系统项目。当Java 7的目标被放弃时，JSR-277和JSR-294从2012年开始被冻结，由JSR-376代替。</p>
<p>OSGi和模块化Java的另一个关联能在JSR-291中找到，“Java SE的动态组件支持”（本质上是OSGi服务平台发布的第四版）。JSR-291参考了JSR-277，作为最早的Java模块系统，来主动区分两者之间的不同作用：JSR-277关注Java中使用的静态模块定义，而JSR-291关注运行时动态组件的加载和卸载。</p>
<p>最后，JSR-376也参考了OSGi，放弃它作为有效解决方案的主要原因是因为它的范围远远大于Java框架模块系统规范。</p>
<p>综上所述，多数人难以区分新的模块系统和OSGi是正常的。但是，模块系统和OSGi相互补充，用以服务于不同的目标，OSGi作为一个构建于Java之上的框架，在一个持续运行的应用中，创建了一个用以动态管理bundle的环境，而模块系统作为Java本身的新能力能够更紧密更简单的控制静态模块。</p>
<h3 id="java平台模块系统和osgi的区别">Java平台模块系统和OSGi的区别</h3>
<p>为了更好的理解这一点，InfoQ请教了Holly Cummins，《Enterprise OSGi in Action》的作者之一。不过以下内容并不会彻底说清楚Java平台模块系统和OSGi之间的差异，只是为读者提供一个基本的理解关于两者目标的区别。</p>
<p>一方面，Java新的模块系统将会提供更简单的方式在编译时检查包和类的可见性，但是当我们询问OSGi的bundle是否能用相同的方式使用时，Holly表示“答案是相当的复杂”。</p>
<blockquote>
<p>OSGi通过bundle的清单文件来描述依赖，有两种基本方式来创建清单文件:“代码优先”和“清单文件优先”。代码优先方式（使用bnd工具，和maven bundle插件），并不会在编译时检查依赖列表，实际上它是在编译时生成的。在通常方式的编译过程中，工具会根据编译时的依赖统计出运行时的依赖。另一种方式是清单文件优先，通过Eclipse PDE工具来使用。在这种方式中，依赖被声明在清单文件中，Eclipse IDE会使用这份清单文件统计出你的代码能访问的类，并高亮缺失的依赖。PDE命令行构建和一个叫做Tycho的Maven插件，都会在编译时检查依赖关系。但是，请注意OSGi自己并不会检查可见性，而是通过PDE自带的工具；因为不是所有使用PDE的团队都会使用其中的工具，有时候可能会在编译时造成缺少依赖。</p>
</blockquote>
<p>新的模块系统的另一个重要方面是能够限定指定包对模块的导出，这是很有用的当一批相互关联的模块需要互相访问，但却不能访问其他内容时。正如Mark Reinhold在报告中指出的，这能够用以下语法来实现：</p>
<pre><code class="language-java"> module java.base {
   ...
   exports sun.reflect to
       java.corba,
       java.logging,
       java.sql,
       java.sql.rowset,
       jdk.scripting.nashorn;
}
</code></pre>
<p>OSGi最初并没有这个能力，但增加之后能够让它比模块系统的目标走的更远。Holly解释道，“所有的bundle都能够注册成一个解析拦截器，用以过滤匹配，因此包只会暴露给指定的bundle。当导出声明了确定的元数据的包给bundle时，你能够使用相同的机制来预测，或者只是在每周二疯狂的导出包”。</p>
<p><a href="http://www.infoq.com/news/2015/10/java-state-module-system">查看英文原文</a>
<a href="http://www.infoq.com/cn/news/2015/11/java-state-module-system">InfoQ译文地址</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maven Filters 使用]]></title>
        <id>https://JadeQin.github.io//post/maven-filters</id>
        <link href="https://JadeQin.github.io//post/maven-filters">
        </link>
        <updated>2015-06-19T02:07:41.000Z</updated>
        <content type="html"><![CDATA[<h1 id="场景">场景</h1>
<p>在项目开发过程中，经常会遇到部署到不同的环境中，比如本地、测试、正式等，如果每次都人为修改相关内容，很容易误操作，也不方便记录。
Maven本身提供了一种解决方案，能够在编译过程中，对相关内容进行修改。</p>
<h1 id="方案">方案</h1>
<ul>
<li>在build中对resource开启filter</li>
</ul>
<pre><code>&lt;resources&gt;
  &lt;resource&gt;
    &lt;directory&gt;${basedir}/src/main/resources&lt;/directory&gt;
    &lt;filtering&gt;true&lt;/filtering&gt;
  &lt;/resource&gt;
&lt;/resources&gt;
</code></pre>
<ul>
<li>定义profile，指定默认的profile</li>
</ul>
<pre><code>&lt;profiles&gt;
  &lt;profile&gt;
    &lt;id&gt;test&lt;/id&gt;
    &lt;activation&gt;
      &lt;activeByDefault&gt;true&lt;/activeByDefault&gt;
    &lt;/activation&gt;
    &lt;properties&gt;
      &lt;profile.env&gt;test&lt;/profile.env&gt;
    &lt;/properties&gt;
  &lt;/profile&gt;

  &lt;profile&gt;
    &lt;id&gt;produce&lt;/id&gt;
    &lt;properties&gt;
      &lt;profile.env&gt;produce&lt;/profile.env&gt;
    &lt;/properties&gt;
  &lt;/profile&gt;
&lt;/profiles&gt;
</code></pre>
<ul>
<li>在build中定义filter</li>
</ul>
<pre><code>&lt;filters&gt;
  &lt;filter&gt;${basedir}/profiles/${profile.env}.properties&lt;/filter&gt;
&lt;/filters&gt;
</code></pre>
<ul>
<li>定义需要修改的内容，比如数据库连接地址</li>
</ul>
<pre><code>db.server=***
</code></pre>
<p>然后在项目的属性文件中，将原来定义的地址替换为<code>${db.server}</code></p>
<p>这样，maven在编译时，会根据<code>-P</code>指定的profile或者默认的profile中定义的属性值，替换成实际内容，避免了人为修改的情况。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[安装Docker Registry]]></title>
        <id>https://JadeQin.github.io//post/install-docker-registry</id>
        <link href="https://JadeQin.github.io//post/install-docker-registry">
        </link>
        <updated>2015-06-10T23:07:59.000Z</updated>
        <content type="html"><![CDATA[<h1 id="镜像库搭建">镜像库搭建</h1>
<p>docker在pull镜像的时候，默认是从docker.io的公共库里面去获取，一个是受限于网络，另一个是不便于公司内部使用，因此需要搭建自己的内部镜像库。
有两种方式</p>
<blockquote>
<p>安装二进制包</p>
</blockquote>
<pre><code>yum install docker-registry
</code></pre>
<blockquote>
<p>直接使用基于Docker的registry镜像</p>
</blockquote>
<pre><code>docker run -d -e SETTINGS_FLAVOR=dev -e STORAGE_PATH=/tmp/registry -v  ~/docker-registry/registry:/tmp/registry  -p 5000:5000 registry
</code></pre>
<p>这样会在<code>~/docker-registry/registry</code>下存放自己的镜像内容，同时开放5000端口供外部访问。</p>
<p>为了便于管理镜像，还可以安装一个公开的registry的管理界面，基于registry v1接口做的。</p>
<pre><code>docker run -d
  -e ENV_DOCKER_REGISTRY_HOST=172.19.103.36
  -e ENV_DOCKER_REGISTRY_PORT=5000
  -p 8080:80
  konradkleine/docker-registry-frontend
</code></pre>
<p>这样可以很方便的查看镜像库里面的内容。</p>
<h1 id="镜像库使用">镜像库使用</h1>
<p>Docker对于镜像地址的识别有点诡异，如果使用域名方式访问的时候，域名不能使用单个单词的方式，因为单个单词会被识别为docker.io上的Repositorie，必须要带<code>.</code>。</p>
<p>如果要把本地build的镜像上传到库里</p>
<pre><code>docker push DOCKER_REGISTRY_HOST:DOCKER_REGISTRY_PORT/NAME
</code></pre>
<p>这样会导致本地使用的时候，镜像名字太长，所以可以本地对镜像打一个短名字的tag，方便使用</p>
<pre><code>docker tag DOCKER_REGISTRY_HOST:DOCKER_REGISTRY_PORT/NAME SHORT_NAME
</code></pre>
<p>当要镜像库里面的镜像的时候</p>
<pre><code>docker pull DOCKER_REGISTRY_HOST:DOCKER_REGISTRY_PORT/NAME
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accumulo Compact]]></title>
        <id>https://JadeQin.github.io//post/accumulo-compact</id>
        <link href="https://JadeQin.github.io//post/accumulo-compact">
        </link>
        <updated>2015-06-03T23:07:46.000Z</updated>
        <content type="html"><![CDATA[<p>Accumulo中，Tablet Server用于处理数据业务，table有可能分成多个tablet，每个tablet在HDFS中对应着一个目录，目录里面放着具体的数据文件。</p>
<p>为了避免tablet中的小文件数过多导致的查询性能降低，Accumulo会定期的合并文件，里面涉及到相关参数。</p>
<p><em>table.compaction.major.ratio</em>
ratio会影响合并后的文件数，增加该值会降低合并操作，其工作原理如下：
如果某一批文件的文件大小总和大于ratio*最大的文件大小，那么就合并这一批文件，否则去掉最大的文件，对剩下的文件做相同操作。
所以radio越大，则文件数越多。</p>
<p><em>tserver.compaction.major.concurrent.max</em>
<em>tserver.compaction.minor.concurrent.max</em>
进行合并操作的线程数大小。</p>
<p><em>table.file.max</em>
如果当前文件数已经达到最大值，则每次将MemTable中的数据写入HDFS的时候，都会触发一次合并操作（merging minor compaction），将数据写入最小的文件中，从而导致吞吐量的下降。</p>
<p>除了通过配置值自动完成，还可以通过手动执行的方式来完成。在shell中执行
flush，将MemTable中的数据写入HDFS
compact，合并文件</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accumulo Write]]></title>
        <id>https://JadeQin.github.io//post/accumulo-write</id>
        <link href="https://JadeQin.github.io//post/accumulo-write">
        </link>
        <updated>2015-05-28T23:07:37.000Z</updated>
        <content type="html"><![CDATA[<h1 id="写入">写入</h1>
<p>TabletServer接收到写入数据后，先写入WAL（Write-Ahead Log），然后将数据写入内存中的一个排序结构（MemTable）。当MemTable的数据量达到一个阈值的时候，将数据写入HDFS，并创建一个新的MemTable。</p>
<p>写往HDFS中的数据存储格式是RFile（Relative Key File），是Accumulo自己的一种文件格式，是一种ISAM文件（Indexed Sequential Access Method）。</p>
<h1 id="读取">读取</h1>
<p>当读取数据时，TabletServer在MemTable和RFile的内存索引中做二分查找，然后对查找结果做合并排序。</p>
<h1 id="minor-compaction">Minor Compaction</h1>
<p>当MemTable中的数据达到阈值时，数据会以文件形式写入HDFS，这个过程就是Minor Compaction。</p>
<h1 id="major-compaction">Major Compaction</h1>
<p>Tablet Server会周期性的合并RFile，将多个文件合并成一个文件，并在Garbage Collector的时候，删除原文件。</p>
<h1 id="garbage-collector">Garbage Collector</h1>
<p>Accumulo会对存储在HDFS中的文件做周期性扫描，删除不再使用的文件。多个GC服务器会选举leader，仅有leader做GC。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Java 对象序列化]]></title>
        <id>https://JadeQin.github.io//post/serialize-pojo</id>
        <link href="https://JadeQin.github.io//post/serialize-pojo">
        </link>
        <updated>2015-05-17T23:07:27.000Z</updated>
        <content type="html"><![CDATA[<h1 id="目的">目的</h1>
<p>通过对比apache Avro和jackson json的序列化方式及效率，找寻一个比较合适的动态序列化方式。
项目地址：<a href="https://github.com/loveai88/serial-benchmark/tree/master">serial-benchmark</a></p>
<h1 id="avro的序列化">Avro的序列化</h1>
<p>Avro本身是从Hadoop项目中独立出来的一个项目，主要用于序列化和rpc，想起以前还有一次面试的时候问，为什么Hadoop要自己做一套序列化和rpc方案，而不用现有的。
Avro相较于Thrift而言，最大的特点应该在于可以运行时改变，而不是必须要有声明类的参与。</p>
<h1 id="jackson-json序列化">jackson json序列化</h1>
<p>jackson json除了基本的pojo和json的转换之后，还通过多种dataformat的形式，提供了json和其他编码格式的转换，比如avro。</p>
]]></content>
    </entry>
</feed>